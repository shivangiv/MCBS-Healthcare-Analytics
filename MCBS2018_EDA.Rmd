---
title: "ALY6160_GroupProject"
author: "Catherine Richard, Shivangi Vashi"
date: "2/6/2021"
output:
  html_notebook: default
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```
### Cost Burden 


```{r}
library(data.table) 
library(tidyverse)
library(dplyr)
library(ggplot2)
library(skimr)
library(randomForest)
library(pROC)
library(caret)
library(e1071)
library(class)

fall18<-fread("MCBSPUF18/puf2018_1_fall.csv") 
fall18<-as.data.frame(fall18)
summer18<-fread("MCBSPUF18/puf2018_3_summer.csv")


fall18_reformatted <-fread("MCBSPUF18/PUF_1_FALL_reformatted.csv")


#ageclass=ADM_H_MEDSTA, insurance_type: Adm_op,dual etc

```




```{r}
dim(fall18)

fall18<-fall18[,c(1,4:239)]
summer18<-summer18[,c(1,4:37)]

dim(fall18)


# colnames(fall18)
head(fall18)

fall18_reformatted  <- fall18_reformatted  %>%
 select(-SURVEYYR, -VERSION, -PUF_ID)
```



### Data Dictonary
Demograhpics:

DEM_AGE: 1: disabled, 2: 65-74 3: elderly >=75 

new col: 'ageclass' 1: disabled 2: elderly

DEM_SEX: 1:male 2: female

DEM_RACE: 1:non hisp white 2: non hisp black 3: hisp 4: other

DEM_EDU: 1: no hs 2: hs / vocational/tech/business
3: more than hs

DEM_MARSTA: 1: married 2: widowed 3: divorced/separated 4:never married

new col 'marital_status': 1: married 2: widowed 3: single
DEM_INCOME: 1: <25k, 2:>=25k

DEM_CBSA: 1: metro 2: non-metro


ACC_HCDELAY: 1: yes 2:no

### Preprocessing


```{r}

#Creating new columns Ageclass and marital status
fall18<-fall18%>%
  mutate(ageclass=ifelse(fall18$DEM_AGE==1,1,2))%>%
  mutate(
    marital_status=ifelse(DEM_MARSTA==1,1,
                          ifelse(DEM_MARSTA==2,2,
                                 ifelse(DEM_MARSTA%in%c(3,4),3,0)
                                 )
                          )
        )
fall18<-fall18%>%select(-DEM_MARSTA)
fall18<-fall18%>%select(-DEM_AGE)
#checking unieuq values for each variable
# lapply(fall18[,-1], unique)
#change char to num
fall18<-fall18%>%mutate_if(is.character,as.numeric) 
summer18<-summer18%>%mutate_if(is.character,as.numeric) 
#Clean non response
fall18<-fall18%>%
      mutate_all(~ replace(.,.%in%c("D","R","",""), 99))%>%mutate_all(~replace_na(.,99))



summer18<-summer18%>%
      mutate_all(~ replace(.,.%in%c("D","R",""), 99))%>%mutate_all(~replace_na(.,99))
dim(fall18)

# lapply(fall18,typeof)


```
Data Dictionary:

ADM_H_MEDSTA: 1: elderly or 2: disabled or  3: unknown
ADM_H_GHPSW: 1: anyMA 0: no
ADM_H_PDRS: retiree drug subsidy(RDS) 1:not subsidized, 2: sub part year 3: sub full year, .:N/A, missing

insurance supplement type hierarchy being "Medicaid">"MAdv">"FFSwESI">"FFSwSelf">"FFSnoSupp"*/

Alternate programming:
1- medicaid
2- medicare advantage
3-FFS with EmpSponsored insurance
4-FFS with self
5-only ffs
99-missing
```{r}
#Insurance Type
fall18<-fall18%>%
            mutate(ins_type=case_when(
                  ADM_DUAL_FLAG_YR%in%c(2,3) ~1 ,
                  INS_D_MADV==1 ~2,
                  INS_D_PVESI==1&ADM_FFS_FLAG_YR%in%c(2,3)~3,
                  INS_D_PVSELF==1&ADM_FFS_FLAG_YR%in%c(2,3)~4,
                  ADM_FFS_FLAG_YR%in%c(2,3)&INS_D_MADV==2&INS_D_PVESI==2&INS_D_PVSELF==2~5,
                    TRUE ~99))



```


```{r}

#Removing NAs from variables of interest if %NA is small
removeNA<-function(v,df){
  x<-sum(df[v]==99)/nrow(df)
  if(x<0.01&&x!=0){
   df<-df%>%filter(df[v]!=99)
  }
  return (df)
}

# fall18<-removeNA(fall18[,50],fall18)
# nacols<-colnames(fall18[,c(38:40,42:43,46:48,50:51)])

dim(fall18)
for(i in names(fall18)){
  fall18<-removeNA(i,fall18)
}

dim(fall18)
# 
# 
# fall18<-fall18[fall18$ACC_HCDELAY!=99,]
# fall18<-fall18[fall18$ACC_HCTROUBL!=99,]
# fall18<-fall18[fall18$ACC_PAYPROB!=99,]
# fall18<-fall18[fall18$DEM_EDU!=99,]

```


1-yes 
2-no
99-missing

```{r}

#Column names for checking comorbidities

cardio<-c("HLT_OCARTERY","HLT_OCMYOCAR","HLT_OCCHD","HLT_OCCFAIL","HLT_OCHRTCND")
arthritis<-c("HLT_OCARTHRH","HLT_OCARTHOT","HLT_OCOSARTH")
cancer<-c("HLT_OCCANCER","HLT_OCCSKIN")
cvd<-c("HLT_OCARTERY","HLT_OCMYOCAR","HLT_OCCHD","HLT_OCCFAIL","HLT_OCHBP","HLT_OCSTROKE")
mentpsych<-c("HLT_OCPSYCHO","HLT_OCDEPRSS")

#Function to flag 1 if patient experiences any of the conditions
flag<-function(cols){
  f = ifelse(apply(fall18[cols]!=1&fall18[cols]!=2, 1, all), 99, 
                               ifelse(apply(fall18[cols]==1,1,any),1,0))
  return(f)
} 


flags<-matrix(nrow=11432, ncol = 7)
flags[,1]<-flag(cardio)
flags[,2]<-flag(arthritis)
flags[,3]<-flag(cancer)
flags[,4]<-flag(cvd)
flags[,5]<-flag(mentpsych)
flags[,6]<-ifelse(fall18$HLT_OCBETES==1,1,0)
flags[,7]<-ifelse(fall18$HLT_OCEMPHYS==1,1,0)


fall18<-fall18%>%mutate(totalcomorb =
                          ifelse(apply(flags!=99&flags!=2, 1, any),
                                 rowSums(flags), 0)
                        )
fall18<-select(fall18,-c(all_of(cardio),all_of(cancer),all_of(arthritis),all_of(cvd),all_of(mentpsych),"HLT_OCBETES","HLT_OCEMPHYS"))

#Remove unwanted variables from the environment
rm(cardio,cancer,arthritis,cvd,mentpsych,flags)

# fall18<- fall18[,-grep(pattern="^HLT",colnames(fall18))]

```


### Creating a variable to flag whether the person experiences cost burden

ACC_HCDELAY- last year ever delay getting hc due to cost
ACC_PAYPROB- prob paying medical bills

(additional)
RXS_DELAYRX- how often delay due to cost: 1-often 2-sometimes 3-never
RXS_NOFILLRX-How often not get Rx because of cost  
/ 
FIS_FOODLAST, FIS_AFFDMEAL, FIS_SKIPMEAL
 
 if delay=1 and payprob=1     then                          1-both
 if delay=1 and payprob=2     then                          2-only delay
 if delay=2 and payprob=1     then                          3-only payprob
 if (delay=1 and payprob=2) or (delay=2 and payprob=1) then 4- either delay or payprob but not both
 
```{r}
#Deleting ACC_PAYOVRTM and ACC_COLAGNY since they only apply if ACC_PAYPROB=1
fall18<-fall18%>%select(-c(ACC_COLAGNCY,ACC_PAYOVRTM))


fall18<-fall18%>%mutate(costburden=case_when((ACC_HCDELAY==1 | ACC_PAYPROB==1)  ~1,
                                             TRUE                               ~0))

# fall18$costburden<-as.factor(fall18$costburden)

fall18<-select(fall18,-c("ACC_HCDELAY","ACC_PAYPROB"))

```


### Exploratory Data Analysis

#### Describing the population

The first plot describes the distribution of disabled and elderly groups by their insurance type.

The second describes the breakdown of whether a beneficiary experiences delay in healthcare due to cost by the age class.
```{r}
library(ggpubr)
#Insurance type by age class
fall18 %>%
  group_by(ins_type, ageclass) %>%
  summarise(population = n()/sum(PUFFWGT)) %>%
  ggplot(aes(x=ins_type, y=population, fill=ageclass))+geom_bar(stat="identity")


# Age groups suffering from hcdelay
g1<-fall18 %>%
  group_by(ageclass,ACC_HCDELAY,PUFFWGT) %>%
  summarise(population=PUFFWGT/sum(fall18$PUFFWGT))%>%
  ggplot( aes(x = ageclass, y = population, fill = ACC_HCDELAY)) + 
  geom_col(position="fill")+xlab("1: Disabled | 2: Elderly")

g2<-fall18 %>%
  group_by(ageclass,ACC_PAYPROB,PUFFWGT) %>%
  summarise(population=PUFFWGT/sum(fall18$PUFFWGT))%>%
  ggplot( aes(x = ageclass, y = population, fill = ACC_PAYPROB)) + 
  geom_col(position="fill")+xlab("1: Disabled | 2: Elderly")


ggarrange(g1,g2)

rm(g1,g2)
```

Breakdown of HCDELAY and PAYPROB by insurance type
```{r}
#Population suffering delaying care due to cost
g1<-fall18 %>%
  group_by(ins_type, ACC_HCDELAY) %>%
  summarise(population = PUFFWGT) %>%
  ggplot(aes(x=ins_type, y=population, fill=ACC_HCDELAY))+geom_bar(stat="identity")

g2<-fall18 %>%
  group_by(ins_type, ACC_PAYPROB) %>%
  summarise(population = PUFFWGT) %>%
  ggplot(aes(x=ins_type, y=population, fill=ACC_PAYPROB))+geom_bar(stat="identity")

ggarrange(g1,g2)

rm(g1,g2)

```


```{r}

df<-data.frame(table(fall18$ins_type, fall18$ACC_HCDELAY))
names(df) <- c("ins_type","ACC_HCDELAY","Count")
#df$population<-fall18$PUFFWGT

ggplot(fall18,aes(x = ageclass,fill=factor(ACC_HCDELAY)))+geom_bar(position = "fill")



fall18%>%
  filter(fall18$MA_MADVYRS!=99)%>%
  ggplot(aes(x =MA_MADVYRS,fill = factor(ACC_HCDELAY), y = PUFFWGT))+
  stat_summary(geom = "col", position = "dodge", fun = sum)




```

1- black 2- white 3-hispanic 4-other
```{r fig.width=20, message=FALSE, warning=FALSE}
library(GGally)



 colnames(fall18)

# ggparcoord(fall18,columns=c(218:219,221,38:39,41:42,45,47,46,50),groupColumn =222,scale="globalminmax")

filter(fall18[,-c(1:37,52:66,138:163,217)],costburden==1)%>%
  ggparcoord(groupColumn =143,scale="globalminmax")

# ggsave("parcord.png",width = 50,height = 40,limitsize = FALSE)


```

Income under Federal Poverty Level by Years enrolled in Medicare Advantage

Distribution of income, poverty line

DEM_INCOME: 1: <25k, 2:>=25k

Poverty Line:
"1: <=100% of the Federal Poverty Level" 
"2: >100% and <=120% of the Federal Poverty"
"3: >120% and <=135% of the Federal Poverty"
"4: >135% and <=200% of the Federal Poverty"
"5: >200% of the Federal Poverty Level"



```{r}


#Income below Federal Poverty level by yeats in MedADV
fall18[fall18$MA_MADVYRS!=99,]%>%
  group_by(MA_MADVYRS, DEM_IPR_IND) %>%
  summarise(population = sum(PUFFWGT)) %>%
  ggplot(aes(x =DEM_IPR_IND , fill =MA_MADVYRS , y =population)) +
  geom_col()

fall18%>%
  group_by(DEM_INCOME, DEM_IPR_IND) %>%
  summarise(population = sum(PUFFWGT)) %>%
  ggplot(aes(x =DEM_IPR_IND , fill =as.factor(DEM_INCOME), y =population)) +
  geom_col()+scale_fill_discrete(labels=c('1:<$25,000', '2:>=$25,000'))

#Income below Poverty Line by Insurance Type
fall18%>%
  group_by( DEM_IPR_IND,ins_type) %>%
  summarise(population = sum(PUFFWGT)) %>%
  ggplot(aes(x =DEM_IPR_IND , fill =factor(ins_type) , y =population)) +
  geom_col()+scale_fill_discrete(labels=c('1: Medicare','2: Medicare Adv','FFS with ESI','FFS with Self','Only FFS'))


```

#### Correlation Analysis

Since the drug coverage and food insecurity questions do not affect our analysis, we can exclude them from our models.
```{r}

fallsum<-inner_join(fall18,summer18,by="PUF_ID")
colnames(fallsum)
mutate_all(fallsum[,c(38:39,41:42,45:47,49:50,216,218:224,245:247,250:251,253)],as.numeric)%>%cor()%>%ggcorrplot::ggcorrplot()
write.csv(fallsum, file = "WeightedAsthma.csv")

```


#### Modelling
```{r}
#Splitting the data into 70% train and 30% test
library(Matrix)
set.seed(123)

fall18<-fall18%>%select(-c("PUF_ID","PUFFWGT"))
label<-as.matrix(as.numeric(fall18$costburden))
sparse_mat<-sparse.model.matrix(costburden~.,data=fall18)


fall18<-fall18%>%select(-c("costburden"))


#splitting data into train and test
train_index <- sample(seq_len(nrow(fall18)),size = floor(0.70*nrow(fall18)))


train<-fall18[train_index,]
test<-fall18[-train_index,]
sparsetrain<-sparse_mat[train_index,]
sparsetest<-sparse_mat[-train_index,]

train_label<-label[train_index,]
test_label<-label[-train_index,]


s1<-list(sparsetrain,train_label)
s2<-list(sparsetest,test_label)

```





```{r}

library(xgboost)


# Parameters for the xgboost model
params <- list(
  # logistic model
  "objective"           = "binary:logistic",
  #learning rate
  "eta"                 = 1,
  
  #depth of tree
  "max_depth"           = 5, 

  
  # the min loss value require to split
  "gamma"               = 0.70,
  
  # fraction of observations to be included in each tree 
  # generally varies from 0.5-1
  "subsample"           = 0.75,
  
  # fraction of column to be randomly sample in each tree
  "colsample_bytree"    = 0.70,
  
  # regularization coefficients
  "alpha"               = 2e-05,
  "lambda"              = 10 
) 


X <- xgb.DMatrix(as.matrix(train), label = train_label)
X2 <- xgb.DMatrix(data=as.matrix(test),label=test_label)

model1 <- xgboost(data = X,max.depth = 4, 
                      eta = 1, 
                      nthread = 2, 
                      nround = 100, 
                      objective = "binary:logistic")


model2 <- xgboost(data = X,
                      nround = 100, 
                      params = params)


epochs <- 20

model3<-xgboost(data = s1[[1]], 
                     label = s1[[2]],
                      max.depth = 2, 
                      eta = 1, 
                      nthread = 2, 
                      nround = epochs, 
                      objective = "binary:logistic")

```


```{r}



score_model <- function(model, 
                        epoch, 
                        data, 
                        datasetname) {
  pred <- predict(model, 
                  newdata = data[[1]], 
                  ntreelimit = epoch)
  
  acc <- mean(data[[2]] == 
                ifelse(pred>=0.5,
                       1.0,
                       0.0))
  dev <- sigr::calcDeviance(pred, 
                            ifelse(data[[2]]>=0.5,
                                   TRUE,
                                   FALSE))
  auc <- sigr::calcAUC(pred, 
                       ifelse(data[[2]]>=0.5,
                              TRUE,
                              FALSE))
  data.frame(dataset = datasetname,
             epoch = epoch, 
             accuracy = acc,
             mean_deviance = dev/nrow(data[[1]]),
             AUC = auc,
             stringsAsFactors = FALSE)
}

score_model_trajectory <- function(model, 
                                   epochs, 
                                   data, 
                                   datasetname) {
  evals <- lapply(epochs,
                  function(epoch) {
                    score_model(model, 
                                epoch, 
                                data, 
                                datasetname)
                  })
  r <- dplyr::bind_rows(evals)
  colnames(r) <- paste(datasetname, 
                       colnames(r), 
                       sep = "_")
  r
}

```


```{r}

# eval<-list()
# 
# 
# eval[[1]] <- 
#   cbind(
#     score_model_trajectory(model, 
#                            100, 
#                            s1, 
#                            "train"),
#     score_model_trajectory(model1, 
#                            100, 
#                            s2, 
#                            "test"))
# 
# eval[[3]] <- 
#   cbind(
#     score_model_trajectory(model3, 
#                            epochs, 
#                            s1, 
#                            "train"),
#     score_model_trajectory(model3, 
#                            epochs, 
#                            s2, 
#                            "test"))
# cols <- c("train_epoch", "train_accuracy", 
#           "train_mean_deviance", "train_AUC", 
#           "test_accuracy", "test_mean_deviance", 
#           "test_AUC")
# eval <- as.data.frame(t(eval))



# cT <- dplyr::tribble(
#   ~measure,                 ~training,             ~validation,
#   "minus mean deviance",    "train_mean_deviance", "train_mean_deviance",
#   "accuracy",               "train_accuracy",      "test_accuracy",
#   "AUC",                    "train_AUC",           "test_AUC"
# )

# 
# WVPlots::plot_fit_trajectory(eval,
#                     column_description = cT,
#                     epoch_name = "train_epoch",
#                     needs_flip = "minus mean deviance",
#                     pick_metric = "minus mean deviance",
#                     title = "xgboost performance trajectories")



```


```{r fig.height=10}
importance <- xgb.importance(colnames(X), model = model1)
x<-xgb.ggplot.importance(top_n(importance,25))

x
importance$Feature[1:25]

# predicting reordered values from test dataset
test$costburden <- predict(model, X2)


#Test error 
p<-test$costburden
err <- mean(as.numeric(p > 0.5) != train_label)
print(paste("test-error=", err))
```




###Random Forest


```{r}
write.csv(fallsum, file = "fallsum.csv")

fallsum <-fread("fallsum.csv") 


set.seed(666)

#set.seed(666)

rawdata <- fallsum

traindata <-rawdata[sample(1:nrow(rawdata),round(0.8*nrow(rawdata))),]
testdata <-rawdata[-sample(1:nrow(rawdata),round(0.8*nrow(rawdata))),]

#hist(fall18$ACC_HCDELAY)
```


```{r}
treeRF1 <- randomForest(traindata$ACC_HCDELAY ~ .-ACC_HCDELAY, traindata, ntree=100)
treeRF1



#  Mean of squared residuals: 0.0002781541
                    #% Var explained: 99.64

### Tuning ### 

# Tuning parameters:
# number of trees 
# number of variables tried at each split ("mtry") = 85
```

```{r}
#increased number of trees
treeRF2 <- randomForest(traindata$ACC_HCDELAY ~ .-ACC_HCDELAY, traindata, ntree=500)
treeRF2

          # Mean of squared residuals: 0.0001722479
          #           % Var explained: 99.78

for(mtry in 1:33){
  fit <- randomForest(traindata$ACC_HCDELAY ~ .-ACC_HCDELAY, traindata, mtry=mtry, ntree=50)
  #oob.err[mtry]<- fit$err.rate[50]
  pred <- predict(fit,testdata, type = "class")
  cat(mtry," ")
}

#Predict Output
predictedRF <- predict(treeRF2,testdata, type = "class")

# Checking classification accuracy
acctest <- table(predictedRF, testdata$ACC_HCDELAY)

# ROC Curve

treeRF_roc <- roc(testdata$ACC_HCDELAY, as.numeric(predictedRF))
treeRF_roc

# treeRF_roc<-roc(testdata$ACC_HCDELAY,treeRF$votes[,2])

plot(treeRF_roc, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1, 0.2),grid.col=c("green", "red"), max.auc.polygon=TRUE,auc.polygon.col="skyblue", print.thres=TRUE,main='ROC curve')

auc(treeRF_roc)
# Area under the curve:1  -- Overfitted?
####
```

KNN

```{r}
#library(class)
dim(raindata$ACC_HCDELAY)


#Fitting model
fit <-knn(traindata$ACC_HCDELAY ~ .-ACC_HCDELAY, traindata, k=5)
dim(traindata$ACC_HCDELAY)
summary(fit)

#Predict Output
predictedKNN <- predict(fit,x_test)
table(predictedKNN, testdata$ACC_HCDELAY)

# ROC Curve
KNN_roc<- roc(testdata$ACC_HCDELAY, as.numeric(predictedKNN))
KNN_roc

#Area under the curve: 0.8816

plot(KNN_roc, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1, 0.2),grid.col=c("green", "red"), max.auc.polygon=TRUE,auc.polygon.col="Old Lace", print.thres=TRUE,main='KNN ROC curve')

auc(KNN_roc)
```

###SVM
Linear
Radial Basis Function ()
```{r}
#library(caret)
#library(kernlab)
# library(e1071)

traindata[["ACC_HCDELAY"]] <- factor(traindata[["ACC_HCDELAY"]])

fit <-svm(traindata$ACC_HCDELAY ~ .-ACC_HCDELAY, traindata) 
summary(fit)

#Predict Output
predictedSVM <- predict(fit,testdata)
table(predictedSVM, testdata$ACC_HCDELAY)

#predictedSVM    1    2
#          1     87    0
#          2     27 1375

# ROC Curve
SVM_roc<- roc(testdata$ACC_HCDELAY, as.numeric(predictedSVM))
SVM_roc
#Area under the curve: 0.8816

plot(SVM_roc, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1, 0.2),grid.col=c("green", "red"), max.auc.polygon=TRUE,auc.polygon.col="Lavender Blush 3", print.thres=TRUE,main='ROC curve')

auc(SVM_roc)
```


```{r}
# svm_Linear <- train(traindata$ACC_HCDELAY ~ .-ACC_HCDELAY, traindata, method = "svmLinear",
#                  #trControl=trctrl,
#                  preProcess = c("center", "scale"),
#                  tuneLength = 10)

# svm_RBF <-ksvm(x = .-ACC_HCDELAY, y = traindata$ACC_HCDELAY,
# + kernel ="rbfdot", kpar = "automatic",
# + C = 1, epsilon = 0.1)

```


